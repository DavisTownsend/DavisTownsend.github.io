{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this article we are going to use [psutil](https://psutil.readthedocs.io/en/latest/) and [gputil](https://github.com/anderskm/gputil) to monitor our system usage and then send slack notifications based on simple rules we create"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases you may want to look at real time monitoring statistics but for the purposes of this demonstration we will just use historical monitoring data with the following rules for slack notifications:\n",
    "\n",
    "    Rules:\n",
    "    * average CPU or memory utilization over past 36 hours is greater than 90% send red alert\n",
    "    * average CPU or memory utilization over past 36 hours is between 80 and 90% send orange alert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting and Storing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll import all the libraries we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T22:02:47.317685Z",
     "start_time": "2018-05-15T22:02:44.741168Z"
    }
   },
   "outputs": [],
   "source": [
    "import psutil\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import time\n",
    "import os\n",
    "import os.path\n",
    "import csv "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell block illustrates the tabular format that we are going to store our data into parquet files later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T22:02:47.584187Z",
     "start_time": "2018-05-15T22:02:47.526716Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cpu_percent</th>\n",
       "      <th>disk_percent</th>\n",
       "      <th>memory_percent</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29.7</td>\n",
       "      <td>98.0</td>\n",
       "      <td>22.8</td>\n",
       "      <td>2018-05-15 17:02:47.540188</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cpu_percent  disk_percent  memory_percent                  timestamp\n",
       "0         29.7          98.0            22.8 2018-05-15 17:02:47.540188"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestamp = datetime.now()\n",
    "cpu_percent = psutil.cpu_percent(interval=0)\n",
    "memory_percent = psutil.virtual_memory()[2]\n",
    "disk_percent = psutil.disk_usage('/')[3]\n",
    "\n",
    "df = pd.DataFrame({'timestamp':[timestamp],\n",
    "                   'cpu_percent': [cpu_percent],\n",
    "                   'memory_percent': [memory_percent],\n",
    "                   'disk_percent': [disk_percent]})\n",
    "\n",
    "df.head()\n",
    "\n",
    "#df.to_csv('example.parquet', engine='auto', compression='snappy')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate the file structure for our parquet files now which will follow the format of a Parquet dataset that goes down to year directories to month directories and then each day is a separate parquet file. In our script we will be appending to a csv file that gets converted to a parquet file at the end of every day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T22:02:49.956764Z",
     "start_time": "2018-05-15T22:02:49.933262Z"
    }
   },
   "outputs": [],
   "source": [
    "#define some basic functions\n",
    "def root_path():\n",
    "    \"\"\"\n",
    "    OS independent way to return root path\n",
    "    \"\"\"\n",
    "    return os.path.abspath(os.sep)\n",
    "\n",
    "    \n",
    "\n",
    "def create_parquet_directory(dataset_name,base_dir):\n",
    "    \n",
    "    \"\"\"\n",
    "    generates base folder of Parquet dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    if not os.path.exists(os.path.join(base_dir,dataset_name)):\n",
    "        os.makedirs(os.path.join(base_dir,dataset_name))\n",
    "        \n",
    "def create_csv_dump_dir(dataset_name,base_dir):\n",
    "    \n",
    "    \"\"\"\n",
    "    generates base folder for csv dump file\n",
    "    \"\"\"\n",
    "    \n",
    "    if not os.path.exists(os.path.join(base_dir,dataset_name)):\n",
    "        os.makedirs(os.path.join(base_dir,dataset_name))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we just set some global variables and call these functions we just made to create parquet dataset and csv dump directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T22:02:51.144270Z",
     "start_time": "2018-05-15T22:02:51.134774Z"
    }
   },
   "outputs": [],
   "source": [
    "PARQUET_DATASET = 'Monitoring'\n",
    "CSV_DUMP_DIR = 'csv_dump'\n",
    "BASE_DIR = root_path()\n",
    "CSV_NAME = 'daily_dump.csv'\n",
    "CSV_FILENAME = BASE_DIR + CSV_DUMP_DIR + '\\\\'+CSV_NAME\n",
    "\n",
    "#create directories if they dont exist\n",
    "create_parquet_directory(PARQUET_DATASET,BASE_DIR)\n",
    "create_csv_dump_dir(CSV_DUMP_DIR,BASE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we are actually writing out the data, we'll use the csv DictWriter to write out since that's easier for writing out single rows of data at a time. So here I jsut show how the data will be structured for writing out to the daily dump csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T22:02:53.043843Z",
     "start_time": "2018-05-15T22:02:53.029344Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cpu_percent': 24.6,\n",
       " 'disk_percent': 98.0,\n",
       " 'memory_percent': 22.8,\n",
       " 'timestamp': datetime.datetime(2018, 5, 15, 17, 2, 53, 37847)}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestamp = datetime.now()\n",
    "cpu_percent = psutil.cpu_percent(interval=0)\n",
    "memory_percent = psutil.virtual_memory()[2]\n",
    "disk_percent = psutil.disk_usage('/')[3]\n",
    "\n",
    "dic = {'timestamp':timestamp,\n",
    " 'cpu_percent': cpu_percent,\n",
    " 'memory_percent': memory_percent,\n",
    " 'disk_percent': disk_percent}\n",
    "\n",
    "dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For testing we can create some sample rows of data in a test csv file to make sure everything is working. The cell below should create the csv file and write out to it the current monitoring stats we got from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T22:03:11.140715Z",
     "start_time": "2018-05-15T22:03:11.125216Z"
    }
   },
   "outputs": [],
   "source": [
    "file_exists = os.path.isfile(CSV_FILENAME)\n",
    "\n",
    "with open(CSV_FILENAME, 'a') as csvfile:\n",
    "    headers = ['timestamp', 'cpu_percent', 'memory_percent','disk_percent']\n",
    "    writer = csv.DictWriter(csvfile, delimiter=',', lineterminator='\\n',fieldnames=headers)\n",
    "\n",
    "    if not file_exists:\n",
    "        writer.writeheader()  # file doesn't exist yet, write a header\n",
    "\n",
    "    writer.writerow({'timestamp': dic['timestamp'], 'cpu_percent': dic['cpu_percent'], \n",
    "                     'memory_percent': dic['memory_percent'], 'disk_percent': dic['disk_percent']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function I've created below will take the daily dump csv file and convert it to a csv file. We call the function at the bottom of this cell block to check that it's working as intended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T22:07:11.375497Z",
     "start_time": "2018-05-15T22:07:11.304028Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Monitoring\\\\Year=2018\\\\Month=5\\\\15.parquet'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def daily_csv_to_parquet(timestamp,base_dir,parquet_dataset,return_filepath=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    function that gets triggered daily to dump previous day's csv to \n",
    "    a parquet file\n",
    "    \n",
    "    Args:\n",
    "        timestamp: timestamp taken when process triggered\n",
    "        base_dir: base directory of dataset\n",
    "        parquet_dataset: parquet dataset name\n",
    "        return_filepath: can be set to true to return filepath of file that was created\n",
    "    \"\"\"\n",
    "    #get year month and day\n",
    "    year = str(timestamp.year)\n",
    "    month = str(timestamp.month)\n",
    "    day = str(timestamp.day)\n",
    "    \n",
    "    #get directory path \n",
    "    year_path = base_dir + parquet_dataset + '\\\\' + 'Year='+year\n",
    "    month_path = year_path + '\\\\'+'Month='+month\n",
    "    day_path = month_path+'\\\\'+day+'.parquet'\n",
    "    \n",
    "    #create directories for month and year if they dont exist\n",
    "    if not os.path.exists(year_path):\n",
    "        os.makedirs(year_path)\n",
    "        \n",
    "    if not os.path.exists(month_path):\n",
    "        os.makedirs(month_path)\n",
    "    \n",
    "    #convert csv file to parquet file\n",
    "    csv_dump_df = pd.read_csv(CSV_FILENAME)\n",
    "    table = pa.Table.from_pandas(csv_dump_df)\n",
    "    pq.write_table(table, day_path,compression ='snappy')\n",
    "    \n",
    "    if return_filepath:\n",
    "        return day_path\n",
    "    \n",
    "filepath = daily_csv_to_parquet(timestamp,BASE_DIR,PARQUET_DATASET,return_filepath=True)\n",
    "filepath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our final check, we can try to read in the parquet file we just converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T22:07:18.509890Z",
     "start_time": "2018-05-15T22:07:18.481770Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>cpu_percent</th>\n",
       "      <th>memory_percent</th>\n",
       "      <th>disk_percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-05-15 17:02:53.037847</td>\n",
       "      <td>24.6</td>\n",
       "      <td>22.8</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-05-15 17:02:53.037847</td>\n",
       "      <td>24.6</td>\n",
       "      <td>22.8</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-05-15 17:02:53.037847</td>\n",
       "      <td>24.6</td>\n",
       "      <td>22.8</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-05-15 17:02:53.037847</td>\n",
       "      <td>24.6</td>\n",
       "      <td>22.8</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-05-15 17:02:53.037847</td>\n",
       "      <td>24.6</td>\n",
       "      <td>22.8</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2018-05-15 17:02:53.037847</td>\n",
       "      <td>24.6</td>\n",
       "      <td>22.8</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2018-05-15 17:02:53.037847</td>\n",
       "      <td>24.6</td>\n",
       "      <td>22.8</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2018-05-15 17:02:53.037847</td>\n",
       "      <td>24.6</td>\n",
       "      <td>22.8</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2018-05-15 17:06:43.541909</td>\n",
       "      <td>14.5</td>\n",
       "      <td>22.6</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2018-05-15 17:06:45.544619</td>\n",
       "      <td>6.6</td>\n",
       "      <td>22.6</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2018-05-15 17:06:47.547464</td>\n",
       "      <td>18.1</td>\n",
       "      <td>22.6</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2018-05-15 17:06:49.549967</td>\n",
       "      <td>23.8</td>\n",
       "      <td>22.6</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2018-05-15 17:06:51.554262</td>\n",
       "      <td>14.3</td>\n",
       "      <td>22.6</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2018-05-15 17:06:53.557137</td>\n",
       "      <td>7.7</td>\n",
       "      <td>22.6</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2018-05-15 17:06:55.560025</td>\n",
       "      <td>26.6</td>\n",
       "      <td>22.7</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2018-05-15 17:06:57.563119</td>\n",
       "      <td>41.6</td>\n",
       "      <td>22.7</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     timestamp  cpu_percent  memory_percent  disk_percent\n",
       "0   2018-05-15 17:02:53.037847         24.6            22.8          98.0\n",
       "1   2018-05-15 17:02:53.037847         24.6            22.8          98.0\n",
       "2   2018-05-15 17:02:53.037847         24.6            22.8          98.0\n",
       "3   2018-05-15 17:02:53.037847         24.6            22.8          98.0\n",
       "4   2018-05-15 17:02:53.037847         24.6            22.8          98.0\n",
       "5   2018-05-15 17:02:53.037847         24.6            22.8          98.0\n",
       "6   2018-05-15 17:02:53.037847         24.6            22.8          98.0\n",
       "7   2018-05-15 17:02:53.037847         24.6            22.8          98.0\n",
       "8   2018-05-15 17:06:43.541909         14.5            22.6          98.0\n",
       "9   2018-05-15 17:06:45.544619          6.6            22.6          98.0\n",
       "10  2018-05-15 17:06:47.547464         18.1            22.6          98.0\n",
       "11  2018-05-15 17:06:49.549967         23.8            22.6          98.0\n",
       "12  2018-05-15 17:06:51.554262         14.3            22.6          98.0\n",
       "13  2018-05-15 17:06:53.557137          7.7            22.6          98.0\n",
       "14  2018-05-15 17:06:55.560025         26.6            22.7          98.0\n",
       "15  2018-05-15 17:06:57.563119         41.6            22.7          98.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = pq.read_table(filepath)\n",
    "table.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our final loop below, if you run this it will continually run and collect the usage metrics on your PC, dumping them to the csv file during the day, and then at the end of each day, convert the csv file to a parquet file and delete the csv file.\n",
    "\n",
    "You could have this code in a script use a scheduler to set how often you want the process to trigger or set the process to run and control how often it triggers by using time.sleep. Of course I'd recommend the scheduler method but for demonstration purposes I'm just using sleep here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T22:06:59.577725Z",
     "start_time": "2018-05-15T22:06:41.473094Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'C:\\\\csv_dump\\\\daily_dump.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-13585c060f7c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mfile_exists\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCSV_FILENAME\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCSV_FILENAME\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'a'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcsvfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m             \u001b[0mheaders\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'timestamp'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'cpu_percent'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'memory_percent'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'disk_percent'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[0mwriter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDictWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsvfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlineterminator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfieldnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'C:\\\\csv_dump\\\\daily_dump.csv'"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    while True:\n",
    "        #set how often you want this to run\n",
    "        time.sleep(2)\n",
    "        #get current timestamp\n",
    "        timestamp = datetime.now()\n",
    "        time_str = timestamp.strftime(\"%Y-%m-%d %H:%M\")\n",
    "        cpu_percent = psutil.cpu_percent(interval=0)\n",
    "        memory_percent = psutil.virtual_memory()[2]\n",
    "        disk_percent = psutil.disk_usage('/')[3]\n",
    "\n",
    "        dic = {'timestamp':timestamp,\n",
    "         'cpu_percent': cpu_percent,\n",
    "         'memory_percent': memory_percent,\n",
    "         'disk_percent': disk_percent}\n",
    "\n",
    "        file_exists = os.path.isfile(CSV_FILENAME)\n",
    "\n",
    "        with open(CSV_FILENAME, 'a') as csvfile:\n",
    "            headers = ['timestamp', 'cpu_percent', 'memory_percent','disk_percent']\n",
    "            writer = csv.DictWriter(csvfile, delimiter=',', lineterminator='\\n',fieldnames=headers)\n",
    "\n",
    "            if not file_exists:\n",
    "                writer.writeheader()  # file doesn't exist yet, write a header\n",
    "\n",
    "            writer.writerow({'timestamp': dic['timestamp'], 'cpu_percent': dic['cpu_percent'], \n",
    "                             'memory_percent': dic['memory_percent'], 'disk_percent': dic['disk_percent']})\n",
    "            \n",
    "        #check if it's next day, if so dump current file to parquet file and reset csv file\n",
    "        if time_str[-5:] == '00:00':\n",
    "            #convert day's csv file to parquet\n",
    "            daily_csv_to_parquet(timestamp,BASE_DIR,PARQUET_DATASET)\n",
    "            #remove daily dump file so new day file starts\n",
    "            os.remove(CSV_FILENAME)\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print('interrupted!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in and Summarizing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's assume now we have many days worth of monitoring data to look at and we want to create a script to read in this data, analyze it, and post slack notifications about consistently high usage\n",
    "\n",
    "We can read in our entire parquet dataset very easily and convert it to a pandas dataframe, this is shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T22:17:08.947870Z",
     "start_time": "2018-05-15T22:17:08.906566Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cpu_percent</th>\n",
       "      <th>memory_percent</th>\n",
       "      <th>disk_percent</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-05-15 17:02:53.037847</th>\n",
       "      <td>24.6</td>\n",
       "      <td>22.8</td>\n",
       "      <td>98.0</td>\n",
       "      <td>2018</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-15 17:02:53.037847</th>\n",
       "      <td>24.6</td>\n",
       "      <td>22.8</td>\n",
       "      <td>98.0</td>\n",
       "      <td>2018</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-15 17:02:53.037847</th>\n",
       "      <td>24.6</td>\n",
       "      <td>22.8</td>\n",
       "      <td>98.0</td>\n",
       "      <td>2018</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-15 17:02:53.037847</th>\n",
       "      <td>24.6</td>\n",
       "      <td>22.8</td>\n",
       "      <td>98.0</td>\n",
       "      <td>2018</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-15 17:02:53.037847</th>\n",
       "      <td>24.6</td>\n",
       "      <td>22.8</td>\n",
       "      <td>98.0</td>\n",
       "      <td>2018</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            cpu_percent  memory_percent  disk_percent  Year  \\\n",
       "timestamp                                                                     \n",
       "2018-05-15 17:02:53.037847         24.6            22.8          98.0  2018   \n",
       "2018-05-15 17:02:53.037847         24.6            22.8          98.0  2018   \n",
       "2018-05-15 17:02:53.037847         24.6            22.8          98.0  2018   \n",
       "2018-05-15 17:02:53.037847         24.6            22.8          98.0  2018   \n",
       "2018-05-15 17:02:53.037847         24.6            22.8          98.0  2018   \n",
       "\n",
       "                           Month  \n",
       "timestamp                         \n",
       "2018-05-15 17:02:53.037847     5  \n",
       "2018-05-15 17:02:53.037847     5  \n",
       "2018-05-15 17:02:53.037847     5  \n",
       "2018-05-15 17:02:53.037847     5  \n",
       "2018-05-15 17:02:53.037847     5  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read in parquet dataset\n",
    "dataset = pq.ParquetDataset(BASE_DIR+PARQUET_DATASET)\n",
    "dataset_table = dataset.read(nthreads=4)\n",
    "full_df = dataset_table.to_pandas()\n",
    "#convert timestamp to datetime index\n",
    "full_df['timestamp'] = pd.to_datetime(full_df.timestamp)\n",
    "full_df = full_df.set_index('timestamp')\n",
    "full_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now whatever level of granularity we want our time in, we can resample our dataframe to get that. In this case we will resample our dataframe by the hour and use the average of our values as the aggregation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T22:31:02.804215Z",
     "start_time": "2018-05-15T22:31:02.785153Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cpu_percent</th>\n",
       "      <th>memory_percent</th>\n",
       "      <th>disk_percent</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-05-15 17:00:00</th>\n",
       "      <td>21.875</td>\n",
       "      <td>22.7125</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     cpu_percent  memory_percent  disk_percent\n",
       "timestamp                                                     \n",
       "2018-05-15 17:00:00       21.875         22.7125          98.0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_dataset_df = full_df.resample('H').mean()\n",
    "full_dataset_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's recap the logic we need to create:\n",
    "\n",
    "    Rules:\n",
    "    * average CPU or memory utilization over past 36 hours is greater than 90% send red alert\n",
    "    * average CPU or memory utilization over past 36 hours is between 80 and 90% send orange alert\n",
    "    \n",
    "So now that we have the average of our utilization metrics by the hour which is the data format we need in this case, we can implement our logic and post the slack notification accordingly right?\n",
    "\n",
    "But wait, we are forgetting something. Because of the way we designed our daily csv dump logic, it only gets converted to a parquet file at the end of every day. So if we want the most up-to-date data (for ex. past hour data), we will need to read in the csv file as well and merge it with this parquet data for the historical data past 24 hour window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T22:31:50.745691Z",
     "start_time": "2018-05-15T22:31:50.717014Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cpu_percent</th>\n",
       "      <th>memory_percent</th>\n",
       "      <th>disk_percent</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-05-15 17:00:00</th>\n",
       "      <td>21.875</td>\n",
       "      <td>22.7125</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     cpu_percent  memory_percent  disk_percent\n",
       "timestamp                                                     \n",
       "2018-05-15 17:00:00       21.875         22.7125          98.0"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_day_df = pd.read_csv(CSV_FILENAME)\n",
    "cur_day_df['timestamp'] = pd.to_datetime(cur_day_df.timestamp)\n",
    "cur_day_df = cur_day_df.set_index('timestamp')\n",
    "cur_day_df = cur_day_df.resample('H').mean()\n",
    "cur_day_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can just combine our historical parquet file dataset with the current day csv file to get all of our data in one pandas dataframe.\n",
    "\n",
    "However, based on our rules we onl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T22:33:52.939692Z",
     "start_time": "2018-05-15T22:33:52.925163Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cpu_percent</th>\n",
       "      <th>memory_percent</th>\n",
       "      <th>disk_percent</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-05-15 17:00:00</th>\n",
       "      <td>21.875</td>\n",
       "      <td>22.7125</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-15 17:00:00</th>\n",
       "      <td>21.875</td>\n",
       "      <td>22.7125</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     cpu_percent  memory_percent  disk_percent\n",
       "timestamp                                                     \n",
       "2018-05-15 17:00:00       21.875         22.7125          98.0\n",
       "2018-05-15 17:00:00       21.875         22.7125          98.0"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df = full_dataset_df.append(cur_day_df)\n",
    "final_df"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
